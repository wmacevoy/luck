\chapter*{Introduction}
\marginnote{These might easily exist, perhaps as another name, in the literature; I just don't know what it is called.}

The point of these notes is to introduce an idea of ``luck'' that connects mathematical probability with the everyday notion.

As a motivating problem, imagine walking along the beach and asking a random person to toss a tennis ball so that it lands in the sand.  The probability that it lands at some point would depend on the habits of the thrower and the details of the beach, but we can summarize this as some probability distribution, $p(x)=\rho(x) dx$, where $x\in\R^2$ is a suitable coordinate system for the beach in question.  It would almost certainly not be a uniform distribution, and it would almost certainly not be particularly concentrated.

Traditional probability feels uncomfortable here.  The chances of the ball landing at a given point is zero\marginnote{For a continuous probablity distribution such as this, the chance of the ball landing in some small area $dx$ near $x$ is $p(x)=\rho(x) dx$.  But the ball lands at a point, so $dx$ is zero, so the probablity $p(x)=\rho(x) dx$ is zero.}, and so miraculous.  Yet anyone watching this process would only occasionaly be surprized by the outcome.

As common (and mundane, not miraculous) such situations are, the language of statistics seems to have difficulty with the notion.  Nor is it limited to continuous cases, just when there are a lot of possible outcomes.  Such examples lead to non-zero but very small probabilities.\marginnote{The real motivation of this came from the space of passwords a person might choose from, which is an effectively infinite discrete space.}

To distinguish from the more general notion of luck, note that that there is no extrinsic value on an outcome.  To say something is ``lucky'' often means there is some value (different from the probability) associated with outcomes.  However, outcomes that are the most valuable are often the least probable, and outcomes of equal probability ought to be equally lucky.  In the most extreme case of all equally probable outcomes (uniform probability), every outcome should have a luck of $\frac{1}{2}$.  

These observations lead to the following definition of luck:
\begin{quote}
The luck $L(x)$ of an outcome $x$ is the probability of getting any outcome $y$ that is more probable than $x$, plus one-half the probability of getting any outcome $y$ that is equally probable to $x$.
\end{quote}

From the perspective of discussing luck, it is convenient to have few sets: $\Omega(x)$, the outcomes more likely than $x$, and $\omega(x)$, the outcomes equally likely to $x$.

\begin{definition}{Omega.}  $\Omega(x)$ is set of outcomes more likely than $x$:
\begin{equation}
\Omega(x) = \left\{ y \mid p(y)>p(x) \right\} \,.
\end{equation}
We define $|\Omega(x)|$ as the probability an outcome is in $\Omega(x)$,
$|\Omega(x)| = P(y\in\Omega(x))$.  In the discrete case, this is
\begin{equation}
|\Omega(x)| = \sum_{y \in \Omega(x)} p(y),
\end{equation}
and, in the continuous case,
\begin{equation}
|\Omega(x)| = \int_{\Omega(x)} \rho(y)\,dy \,.
\end{equation}
\end{definition}

\begin{definition}{omega.}\marginnote{For the tyical case of many outcomes with different probabilities, $|\omega(x)|$ is small.  For example, $|\omega(x)|=0$ for any multivariate normal distribution.}
$\omega(x)$ is the set of outcomes equally likley to $x$:
\begin{equation}
\omega(x) = \left\{ y \mid p(y)=p(x) \right\} \,.
\end{equation}
Similar to $\Omega(x)$, we define $|\omega(x)|$ as the probability an outcome is in $\omega(x)$,
$|\omega(x)| = P(y\in\omega(x))$.  

In the discrete case, this is
\begin{equation}
|\omega(x)| = \sum_{y \in \omega(x)} p(y),
\end{equation}
and, in the continuous case,
\begin{equation}
|\omega(x)| = \int_{\omega(x)} \rho(y)\,dy \,.
\end{equation}
\end{definition}

With these definitions in place, we define luck mathematically as follows:
\begin{definition}{Luck.}  The luck of an outcome is the probability getting any more likely outcome, plus half the probability of getting any equally likely outcome:
\begin{equation}
L(x)=|\Omega(x)| + \frac{1}{2} |\omega(x)| \,.
\end{equation}
\end{definition}


\subsection{Properties of Luck.}
\begin{itemize}
\item $0 \leq L(x) \leq 1$.  This ranges from no luck to perfect luck.
\item If $L(x)$ is close to 1, then $p(x)$ is comparatively small, and most outcomes would have a higher probability (you are lucky).  
\item If $L(x)$ is close to 0, then $p(x)$ is comparatively large, and most outcomes would have a lower probability (you are unlucky).
\item $E(L)=\frac{1}{2}$.  On average, luck is always 50:50.
\end{itemize}

We are interested in cases which have many possible outcomes with low but somewhat different probabilities (like the tennis ball on the beach).  If the space is well divided (so $\max |\omega|=\max_{x}|\omega(x)|$ is small), then there are other interesting properties of luck:\marginnote{In particular $|\omega(x)|=0$ for the normal, exponential, beta, and gamma distributions.  As a worst-case counterexample, the flattest distribution is the uniform distribution, for which $|\omega(x)|=1$.}
\begin{itemize}
\item $E(f(L))=\int_0^1 f(L) dL+\varepsilon$, where $|\varepsilon| \leq \max|f''| \cdot \max |\omega|^2 / 12$.
\item For $p \geq 1$, $E(L^p)=1/(p+1)-\varepsilon$, $0 \leq \varepsilon \leq p \cdot (p-1) \max |\omega|^2/12$.
\item For $0 \leq a \leq b \leq 1$, $E(L \in [a,b])=b-a + \varepsilon$, $|\varepsilon| \leq \max |\omega|$.
\end{itemize}
The proofs of these come from the midpoint integration rule and thinking about the general case for figure~\ref{fig:arrange} below.

\begin{example}{Coins.}
Suppose we toss 8 fair coins.  The probability of getting exactly $x$ heads out of 8 tosses is given by the binomial distribution
\begin{equation}
p(x)=\frac{8!}{x!(8-x)!}\left(\frac{1}{2}\right)^8 \,.
\end{equation}
What is the luck associated with this distribution?
\begin{table}
\caption{This is arranged in increasing luck (which is decreasing probability). Getting exactly $x=4$ heads is unlucky, requiring only $L=14\%$ luck, while getting $x=0$ or $x=8$ heads is almost $100\%$ luck.}
\begin{tabular}{|c|S[table-format=2.4]|c|S[table-format=2.4]|c|S[table-format=2.4]|c|S[table-format=2.4]|S[table-format=2.4]|}
\multicolumn{1}{c}{$x$} &
\multicolumn{1}{c}{$p(x)$} &
\multicolumn{1}{c}{$\Omega(x)$} &
\multicolumn{1}{c}{$|\Omega(x)|$} &
\multicolumn{1}{c}{$\omega(x)$} &
\multicolumn{1}{c}{$|\omega(x)|$} &
\multicolumn{1}{c}{$L(x)$} \\
\hline
     4 & 0.2734 &  \{\} & 0.0000 & \{4\} & 0.2734 & 0.1367 \\
3 or 5 & 0.2188 & \{4\} & 0.2734 & \{3,5\} & 0.4375 & 0.4922 \\
2 or 6 & 0.1094 & \{3,4,5\} & 0.7109 & \{2,6\} & 0.2188 & 0.8203 \\
1 or 7 & 0.0313 & \{2,3,4,5,6\} & 0.9297 & \{1,7\} & 0.0625 & 0.9609 \\
0 or 8 & 0.0039 & \{1,2,3,4,5,6,7\} & 0.9922 & \{0,8\} & 0.0078 & 0.9961 \\
\hline
\end{tabular}
\end{table}
\begin{figure}
\begin{center}
\includegraphics[width=1.00\linewidth]{graphics/arrange.pdf}
\end{center}
\caption{\label{fig:arrange}Luck to get $x$ heads on $8$ fair coins.  After the probabilities are arranged in decreasing order on the unit interval, $L(x)$ is the center point of equally probable outcomes.}
\label{fig:arrange}
\end{figure}

Luck on average is $\frac{1}{2}$:
\marginnote{For any distribution, $E(L)=\frac{1}{2}$.}
\begin{equation}
E(L)=\sum_{x=0}^8 p(x) \cdot L(x) = \frac{1}{2} \,.
\end{equation}

The second moment should be close to $\frac{1}{3}$:
\marginnote{For any distribution, $E(L^2)=\frac{1}{3}-\varepsilon$, with $0 \leq \varepsilon \leq \max |\omega|^2/6$.

For the coin distribution, the bound is $0 \leq \varepsilon \leq 0.032$, and the actual error $\varepsilon=0.0096$.}
\begin{equation}
E(L^2)=\sum_{x=0}^8 p(x) \cdot L(x)^2 = \frac{1}{3} - 0.0096
\end{equation}

The probability luck is in the middle half is about $\frac{1}{2}$:
\marginnote{For any distribution, $E(L \in [a,b]) = b-a+\varepsilon$, with $|\varepsilon|\leq \max |\omega|$.

For the coin distribution and $[a,b]=[\frac{1}{4},\frac{3}{4}]$, $E(L \in [a,b])=\frac{1}{2}+\varepsilon$ with $|\varepsilon|\leq 0.4375$ as the (poor) error bound, and the actual error of $\varepsilon=-0.0625$}
\begin{equation}
\begin{split}
E(L \in [\frac{1}{4},\frac{3}{4}]) &= \sum_{x=0}^8 p(x) \cdot 
\left\{ \begin{array}{cl} 1 & \text{if $L(x) \in [\frac{1}{4},\frac{3}{4}]$} \\
                          0 & \text{otherwise} 
\end{array} \right\} \\
&= \frac{1}{2} - 0.0625 \,.
\end{split}
\end{equation}
\end{example}
\begin{example}{Normal.}  This is a special case of the multivariate normal we cover in the next section, but working out the details for the one-dimensional case can be illuminating.  We define the one-dimensional normal distribution with mean $\mu$ and variance $\Sigma$ as
\begin{align}
P_{\text{normal}}(x;\mu,\Sigma) &= \frac{e^{-\frac{(x-\mu)^2}{2\Sigma}}}{\sqrt{2\pi\Sigma}} \,,\\
\intertext{where}
\mu &= E(x)\,,\\
\intertext{and}
\Sigma &= E((x-\mu)^2) \,.
\end{align}

First note that $\Omega(x)$ is the open the interval between $x$ and $x$ reflected around $\mu$:
\begin{equation}
\Omega(x)=(\min(x,2\mu-x),\max(x,2\mu-x)) \,,
\end{equation}
and $\omega(x)$ is the endpoints of that interval:
\begin{equation}
\omega(x)=\{x,2\mu-x\} \,.
\end{equation}

Since the 1-dimensional normal distribution is a continuous distribution and $\omega(x)$ is a finite set, $|\omega(x)|=0$, i.e.,
\begin{equation}
|\omega(x)|=\int_{\omega(x)} P_{\text{normal}}(y;\mu,\Sigma) \, dy = 0 \,.
\end{equation}
This means all the luck properties are exact (the error terms are zero), and the $\frac{1}{2}|\omega(x)|$ contributes nothing to the luck of an outcome.

What remains is to calculate luck,
\begin{equation}
L(x)=\int_{\Omega(x)} P_{\text{normal}}(y;\mu,\Sigma) \, dy \,.
\end{equation}
Changing variables to the normalized $z$-score: $z=\sqrt{\Sigma^{-1}}(x-\mu)$, this can be rewritten as
\begin{equation}
L(x)=\int_{-R}^{R} P_{\text{normal}}(y,0,1) \, dy \,,
\end{equation}
where
\begin{equation}
R=|\sqrt{\Sigma^{-1}}(x-\mu)| \,.
\end{equation}

Using $\erf(x)$, defined as\marginnote{$\erf(x)$ is a normalized integral of the $P_{\text{normal}}(x;\mu=0,\Sigma=1/2)$ so that $\erf(0)=0$ and $\erf(\pm \infty)=\pm 1$.}
\begin{equation}
\erf(x) = \frac{2}{\sqrt{\pi}} \int_0^x e^{-y^2} \, dy \,, 
\end{equation}
the luck of an outcome can be written as
\begin{equation}
\label{eq:normal-1d-luck}
L(x)=\erf\left|\frac{x-\mu}{\sqrt{2\Sigma}} \right| \,.
\end{equation}

In the next chapter, where we address the more general multivariate normal case, we obtain the approximation,
\begin{equation}
\label{eq:approx-normal-1d-luck}
L(x) \approx \frac{1}{2} \left[ 1+\erf\left(\left|\frac{x-\mu}{\sqrt{\Sigma}} \right|-\sqrt{\frac{1}{2}}\right)\right] \,.
\end{equation}
Figure~\ref{fig:normal1} compares the exact and approximate result in the 1-d case.

\begin{figure}
\begin{center}
\includegraphics[width=0.75\linewidth]{img/normal1.png}
\end{center}
\caption{Exact (blue, equation~\ref{eq:normal-1d-luck}) vs approximate (red, equation~\ref{eq:approx-normal-1d-luck}) luck for 1-d normal distribution.}
\label{fig:normal1}
\end{figure}
\end{example}
