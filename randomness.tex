\chapter{Application - Testing Randomness}
A popular randomness testing suite is the Dieharder suite, which can be used to validate various random number generators, including custom generators, against a set of established tests of randomness.  Under standard usage, the outcome of each test is a $p$-value, which if is outside a cutoff $|p-1/2|>1/2-\varepsilon/2$, is considered weak ($\varepsilon=0.005$) or fails ($\varepsilon=0.000001$).

The issue is that, assuming the test and source of randomness are correct, the $p$-value is a uniformly distributed value.  This means that $\varepsilon$ of the tests will be labeled weak or failed.  With many tests and random number generators, there should be failures.  This possibility of false negatives makes the results difficult to interpret.  On the other hand, a consistently weak random number generator may produce a poor result consistently, just not weak enough to cross the predetermined weak/fail test, so false positives are also a problem.

We took two luck-based approaches to this.  The first was to create our own test which we could assess using a luck approach, and the second was to take a luck perspective on the built-in dieharder results.

\section{Max64 Statistics}
The first was to design our own test (Max64) with a known exactly computable statistical distribution.  The outcomes of each test is combined using the luck-adjusted $z$-score (\ref{eq:luck-adjusted-z-score}) of individual tests to obtain an overall normal luck score via the repeated use of (\ref{eq:zl-combine}).  This is continued until the number of tests exceeds some limit, or $z_L$ reaches a value that constitutes a statistical proof of failure, i.e., $|z_L|>10$ which should occur less than once in $10^{44}$ trials.

The chosen statistical test is very simple: the distribution of maximum values on a filtered permutation of the bit stream.  This was done because the statistical moments can be computed to high precision (this is crucial, since incorrect moments would result in model failure because of the test, not the source of randomness).  The resulting project is hosted as github.com/wmacevoy/testrng, and is remarkably efficient at determining poor randomness compared to the standard Dieharder suite.

\section{Probability of Maximums}
If taking $S$ independent samples $(x_1,\ldots,x_S)$ uniformly of the numbers $\{1\ldots N\}$, the probability that 
the maximum value is $M$ is given by 
\begin{align}
P(M)=\Prob(\max_{k=1}^{S} x_k = M) &= \left(\frac{M}{N}\right)^S \cdot \left[ 1-\left(\frac{M-1}{M}\right)^S \right]  \\
      &= \frac{1}{N^S} \left(M^S -(M-1)^S\right) \,.
\end{align}

The expected maximum value is given by
\begin{align}
E(M) &= \sum_{M=1}^{N} P(M) M \,, \\
     &= \frac{1}{N^S} \sum_{M=1}^{N} \left[ M^{S+1} -(M-1)^S \cdot ((M-1)+1) \right] \,, \\
     &= \frac{1}{N^S} \left[N^{S+1} - \sum_{M=1}^{N} (M-1)^{S}  \right] \,, \\
     &= N - \sum_{M=1}^{N-1} \left(\frac{M}{N}\right)^S \,, \\
     &= N - \frac{1}{N^S} H(N-1,-S) \,.
\end{align}
Here, we have used the generalized harmonic number $H(n,p)$;
\begin{equation}
H(n,p) = \sum_{k=1}^{n} k^{-p} \,.
\label{eq:generalized-harmonic-number}
\end{equation}

As the number of samples, increase, we expect $M$ to be close to $N$.  It is numerically more stable to consider the statistics of the gap $G$, defined as:
\begin{equation}
G=\frac{N-M}{N} \,.
\end{equation}
Rewriting $E(M)$ for $E(G)=(N-E(M))/N$, we find
\begin{equation}
E(G) = \frac{1}{N^{S+1}} H(N-1,-S) \,.
\end{equation}

A similar calculation for the second moments results in
\begin{align}
\Var{G}&=E((G-E(G))^2) \\
       &=E(G) \cdot (2-E(G) - \frac{1}{N}) -\frac{ 2 H(N-1,-(S+1))}{N^{S+2}}  \,.
\end{align}

In the Max64 test, we are using large values of $N > 2^{40}$, so it is not feasible to use the explicit sum \ref{eq:generalized-harmonic-number}.  Using the approximation,
\begin{equation}
\sum_{k=1}^{n} k^s \approx \frac{1}{s+1} (n+1/2)^{s+1} + \text{const}
\end{equation}
we obtain
\begin{equation}
E(G) \approx g \,.
\end{equation}
and
\begin{equation}
Var(G) \approx (2-g-1/N)g -2h \,.
\end{equation}
where
\begin{align}
g &= \frac{e^{-(S+1)*z}}{S+1} \,, \\
h &= \frac{e^{-(S+2)*z}}{S+2} \,, \\
z &= \frac{1}{2N} \left(1+\frac{1}{4N}\right) \,.
\end{align}
Numerical simulations suggest this is a relative $O(1/N^2)$ approximation.  With accurate moment results to compute the mean and standard deviation, we performed a set of tests using $S=19$ samples and $N=2^{63}$.

\begin{table}
\caption{\label{tab:maxluck}Accumulated normal luck using the Max64 tests for failed reference Dieharder PRNGs and AES\_OFB as strong counterexample.}
\begin{tabular}{|c|r|c|r|}
\multicolumn{1}{c}{Generator} &
\multicolumn{1}{c}{Trials} &
\multicolumn{1}{c}{Outcome} &
\multicolumn{1}{c}{Normal luck} \\
\hline
borosh13 & $3,005$ & lucky &  $\approx 1-10^{-45}$ \\
rand & $11,292$ & unlucky &  $\approx 10^{-45}$ \\
coveyou & $898$ & unlucky & $\approx 10^{-45}$ \\
knuthran & $12,002$ & lucky & $\approx 1-10^{-45}$ \\
ran3 & $3,552$ & lucky & $\approx 1-10^{-45}$ \\
r250 & $95,415$ & lucky & $\approx 1-10^{-45}$ \\
ranlux & $503$ & lucky & $\approx 1-10^{-45}$ \\
ranlux389 & $911$ & lucky & $\approx 1-10^{-45}$ \\
ranlxs0 & $613$ & lucky & $\approx 1-10^{-45}$ \\
ranlxs1 & $387$ & lucky & $\approx 1-10^{-45}$ \\
ranlxs2 & $790$ & lucky & $\approx 1-10^{-45}$ \\
random8-bsd & $19,311$ & lucky & $\approx 1-10^{-45}$ \\
random8-glibc2 & $1,914$ & unlucky & $\approx 10^{-45}$ \\
ranmar & $551$ & lucky & $\approx 1-10^{-45}$ \\
slatec & $487$ & lucky & $\approx 1-10^{-45}$ \\
transputer & $9,450$  & unlucky & $\approx 10^{-45}$ \\
uni & $100$ & lucky & $\approx 1-10^{-45}$ \\
vax & $20,523$ & lucky & $\approx 1-10^{-45}$ \\
waterman14 & $2,512$ & unlucky & $\approx 10^{-45}$ \\
zuf & $271$ & lucky & $\approx 1-10^{-45}$ \\
R\_knuth\_taocp & $16,010$ & lucky & $\approx 1-10^{-45}$ \\
R\_knuth\_taocp2 & $8,254$ & lucky & $\approx 1-10^{-45}$ \\
AES\_OFB & $1,000,000$ & normal & $0.2854$ \\
\hline
\end{tabular}
\end{table}
Table~\ref{tab:maxluck} Constitutes a numerical statistical proof of failure for some reference PRNGS in the Dieharder suite (except AES\_OFB which is provided as a counter example).  The impossible accumulated luck is proof either the model or the source of randomness is wrong.  The last row contrasts this with the AES\_OFB reference generator.  Here, even after 1,000,000 trials, the accumulated luck is unsurprising, which is strong evidence the model is correct.

The testrng tool proves the first 22 generators are not random in about 20 seconds, while getting indeterminate results using the Dieharder suite took about 1 week of CPU time on a faster server.

\section{Luck interpretation of dieharder results}
The second approach is a more qualitative result.  The typical output of a dieharder test is a $p$-value, which should be uniformly distributed if the test is correct and the generator is indistinguishable from random by the given test.  Because there are many tests and many generators, weak/fail results are expected to happen.  This is frustrating because it makes the results more difficult to interpret.  By repeating a test, a consistently weak or failed test is a more reliable outcome.  But that gives little concrete advice to determine how many failures constitute a real failure, nor does it allow for many-almost failures to have any meaning.

To create a more summary proof-of-failure statistic, we ran 10 instances of each known-good test against each algorithmic PRNG, and assumed the $p$-value of each outcome was the outcome of a 1-dimensional normal distribution.  This  allowed for the computation of a $z$-score.\marginnote{Extreme $p=1$ and $p=0$ outcomes were assigned a $z$-score of $\pm 4$, since and error of $10^{-6}$ is possible from the formatting of the results.  Also, the rgb\_minimum tests were ignored since they failed with $p=0$ on every generator}  In this way, for each random number generator, each $p$-value for all tests provided a normal luck $z_L=z-\sqrt{1/2}$ and $d_f=1$. These were accumulated over all 590 tests for the following summary statistics.

\begin{table}
\caption{\label{tab:dieluck}Normal luck estimates using internal dieharder tests summarizing $590$ tests assuming the $p$-value of each test came from a 1-dimensional normal distribution and compared against the Max64 test with the same $|z_L|>10$ cutoff criteria.}
\begin{tabular}{|c|S[table-format=2.1]|r|r|c|c|}
\multicolumn{1}{c}{PRNG} & \multicolumn{1}{c}{$z_L$} & \multicolumn{1}{c}{\#WEAK} & \multicolumn{1}{c}{\#FAIL} & \multicolumn{1}{c}{$|z_L|>10$} & \multicolumn{1}{c}{Max64} \\
\hline
borosh13 & 61.7 & 17 & 435 & lucky & lucky \\
cmrg & 2.4 & 19 & 0 &  & \\
coveyou & 59.6 & 6 & 418 & lucky & unlucky\\
fishman18 & 2.8 & 8 & 0 &  & \\
fishman20 & 5.6 & 14 & 10 & & \\
fishman2x & 2.9 & 16 & 0 &  & \\
gfsr4 & 4.1 & 17 & 1 & & \\
knuthran & 2.4 & 14 & 0 &  & \\
knuthran2 & 3.0 & 13 & 0 &  & \\
lecuyer21 & 4.8 & 10 & 10 & & \\
minstd & 4.5 & 15 & 10 & & \\
mrg & 1.0 & 12 & 0 &  & \\
mt19937 & 2.5 & 13 & 0 &  & \\
mt19937\_1999 & 3.5 & 18 & 0 & & \\
mt19937\_1998 & 1.7 & 12 & 0 &  & \\
r250 & 20.3 & 39 & 50 & lucky & lucky \\
ran0 & 5.4 & 12 & 10 & & \\
ran1 & 1.9 & 7 & 0 &  & \\
ran2 & 1.8 & 8 & 0 &  & \\
ran3 & 50.5 & 9 & 316 & lucky & lucky \\
rand & 58.3 & 18 & 388 & lucky & unlucky\\
rand48 & 5.1 & 17 & 10 & & \\
random128-bsd & 6.6 & 19 & 10 & & \\
random128-glibc2 & 7.8 & 22 & 10 & & \\
random128-libc5 & 6.8 & 25 & 10 & & \\
random256-bsd & 4.0 & 14 & 0 & & \\
random256-glibc2 & 2.7 & 11 & 0 &  & \\
random256-libc5 & 3.9 & 22 & 0 & & \\
random32-bsd & 54.1 & 36 & 323 & lucky & \\
random32-glibc2 & 53.0 & 40 & 312 & lucky & \\
random32-libc5 & 53.4 & 35 & 321 & lucky & \\
random64-bsd & 21.8 & 46 & 59 & lucky & \\
random64-glibc2 & 21.4 & 42 & 57 & lucky & \\
random64-libc5 & 22.3 & 43 & 63 & lucky & \\
\hline
\end{tabular}
\end{table}

\begin{table}
\caption{\label{tab:dieluck}Table~\ref{tab:dieluck} continued.}
\begin{tabular}{|c|S[table-format=2.1]|r|r|c|c|}
\multicolumn{1}{c}{PRNG} & \multicolumn{1}{c}{$z_L$} & \multicolumn{1}{c}{\#WEAK} & \multicolumn{1}{c}{\#FAIL} & \multicolumn{1}{c}{$|z_L|>10$} & \multicolumn{1}{c}{Max64} \\
\hline
random8-bsd & 57.8 & 14 & 390 & lucky & lucky \\
random8-glibc2 & 58.4 & 14 & 391 & lucky & unlucky \\
random8-libc5 & 58.2 & 13 & 390 & lucky & lucky \\
random-bsd & 5.1 & 10 & 11 & & \\
random-glibc2 & 7.1 & 18 & 10 & & \\
random-libc5 & 5.6 & 19 & 10 & & \\
randu & 67.8 & 27 & 497 & lucky & unlucky \\
ranf & 8.0 & 19 & 10 & & \\
ranlux & 3.5 & 16 & 0 & & lucky \\
ranlux389 & 2.7 & 12 & 0 &  & lucky \\
ranlxd1 & 0.9 & 10 & 0 &  &  \\
ranlxd2 & 1.3 & 8 & 0 &  & \\
ranlxs0 & 2.3 & 8 & 0 &  & lucky \\
ranlxs1 & 2.0 & 14 & 0 &  & lucky \\
ranlxs2 & 2.3 & 10 & 0 &  & lucky \\
ranmar & 2.4 & 12 & 0 &  & lucky \\
slatec & 67.4 & 25 & 485 & lucky & lucky \\
taus & 1.4 & 10 & 0 &  & \\
taus2 & 1.4 & 10 & 0 &  & \\
taus113 & 1.8 & 7 & 0 &  & \\
transputer & 62.3 & 6 & 448 & lucky & unlucky\\
tt800 & 1.0 & 9 & 0 &  & \\
uni & 12.9 & 10 & 40 & lucky & lucky \\
uni32 & 10.8 & 12 & 31 & lucky & normal \\
vax & 58.0 & 19 & 384 & lucky & lucky \\
waterman14 & 61.4 & 12 & 437 & lucky & unlucky \\
zuf & 3.3 & 13 & 0 & & lucky \\
ca & 6.6 & 16 & 10 & & \\
uvag & 3.0 & 14 & 0 & & \\
AES\_OFB & 1.5 & 8 & 0 &  & \\
Threefish\_OFB & 3.2 & 20 & 0 & & \\
kiss & 2.6 & 12 & 0 &  & \\
superkiss & 7.7 & 12 & 20 & & \\
R\_wichmann\_hill & 2.1 & 12 & 0 &  & \\
R\_marsaglia\_multic. & 5.7 & 16 & 10 & & \\
R\_super\_duper & 10.5 & 12 & 25 & lucky & \\
R\_mersenne\_twister & 2.1 & 14 & 0 &  & \\
R\_knuth\_taocp & 2.4 & 12 & 0 &  & lucky \\
R\_knuth\_taocp2 & 5.1 & 25 & 0 & & lucky \\
\hline
\end{tabular}
\end{table}
Some notable entries are gfsr4, which failed 1 test but did not achieve a provably surprising normal luck result.  By contrast R\_knuth\_taocp2 failed no tests and had relatively few weak results, but had a much more surprising $z_L$.  Max64 proves the latter is too lucky to be random.  The ranlxs* vs the random32-* entries show neither Dieharder nor Max64 are uniformly better at disproving the randomness of a generator.

If one dedicated a 100 weeks of cpu time to this, it would be interesting to compare a larger data set.  There are many highly unlikely results in the luck summary which are not statistical proofs, however because of the cavalier approach to the extraction of the initial normal luck, the results always have to be made in comparison to a strong generator.  On the other hand, if the Dieharder suite provided $z_L$ and $d_f$ values for each test, it would be a trivial matter to provide a summative provable result.

There are other ways these results could be significantly improved.  First, the actual statistic could be used to obtain a correct $z_L$ score to combine.  Second, the $z_L$ score could be reported instead of inferring it from the $p$-value so that extreme results could be incorporated into the luck estimates with greater accuracy.  Finally, once a fixed number of tests are computed in the suite, the tests should be repeated so long as the contribution to the overall normal luck is monotonic.  For correctly random luck, this introduces a small number of additional runs, but consistently lucky/unlucky trending tests will culminate into a proof of failure.  These results would be much easier to interpret overall.

