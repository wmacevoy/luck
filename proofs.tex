\chapter*{Proofs}


\section{From the introduction}

\begin{theorem}{Range of Luck.} 
For any probability space,
\begin{equation}
0 \leq L(x) \leq 1 \,.
\end{equation}

Proof.  From the definition, $L(x)=|\Omega(x)|+\frac{1}{2} |\omega(x)|$, which is clearly non-negative, and $\Omega(x)$ and $\omega(x)$ are disjoint subsets of the probability space.  Here the $| \cdot |$ notation is the measure of these sets in the probability space, and their union is at most the whole space, so $L(x) \leq |\Omega(x) \cup \omega(x)| \leq 1$.
\end{theorem}

\begin{theorem}{Lucky values.}
If $L(x)$ is close to 1, then $p(x)$ is relatively small and most outcomes have a higher probability (you are lucky).

Proof.  Since $L(x)$ is close to 1, write $L(x)=1-\varepsilon(x)$, where $\varepsilon(x)$ is a small non-negative number.

First, since $\Omega(x)$ and $\omega(x)$ are disjoint subsets of the space of outcomes $X$, $|\Omega(x)|+|\omega(x)| \leq 1$, or 
\begin{equation*}
|\Omega(x)| \leq 1-|\omega(x)| \,.
\end{equation*}

Second, $L(x)=|\Omega(x)|+\frac{1}{2}|\omega(x)|$, which by the first inequality can be bounded as $1-\varepsilon(x)\leq  1-|\omega(x)|+\frac{1}{2}|\omega(x)|$, which can be rearranged as
\begin{equation*}
|\omega(x)| \leq 2 \varepsilon(x) \,.
\end{equation*}

Third, $|\Omega(x)|+\frac{1}{2}|\omega(x)| = 1-\varepsilon(x)$, or $|\Omega(x)|=1-\varepsilon(x)-\frac{1}{2}|\omega(x)|$, which by the second inequality,
\begin{equation}
|\Omega(x)| \geq 1-2 \varepsilon(x) \,.
\end{equation}
Thus at least a $1-2 \varepsilon(x)$ fraction of the probability space have a higher probability of occurring.
\end{theorem}

\begin{theorem}{Unlucky values.}
If $L(x)$ is close to 0, then $p(x)$ is comparatively large, and most outcomes would have a lower probability (you are unlucky).

Proof.  Since $L(x)$ is close to 0, write $L(x)=\varepsilon(x)$, where $\varepsilon(x)$ is a small non-negative number.

First, $L(x)=|\Omega(x)|+\frac{1}{2}|\omega(x)|$, so
\begin{equation*}
\omega(x) \leq 2\varepsilon(x) \,.
\end{equation*}

Second, $|\Omega(x)|+|\omega(x)|=|\Omega(x)|+\frac{1}{2}|\omega(x)|+\frac{1}{2}|\omega(x)| \leq 2\varepsilon(x)$, so
\begin{equation*}
|\Omega(x)|+|\omega(x)| \leq 2\varepsilon(x) \,.
\end{equation*}

Third, $|X-[\Omega(x) \cup \omega(x)]| \geq 1-|\Omega(x) \cup \omega(x)| \geq 1-2\varepsilon(x)$, or 
\begin{equation*}
|X-[\Omega(x) \cup \omega(x)]| \geq 1-2\varepsilon(x) \,.
\end{equation*}
Thus at least a $1-2 \varepsilon(x)$ fraction of the probability space have a lower probability of occurring.
\end{theorem}

\begin{theorem}{On average, luck is always 50:50.}

\begin{equation*}
E(L)=\frac{1}{2} \,.
\end{equation*}

Proof.  This is an application of the next theorem where $f(L)=L$.
\end{theorem}

\begin{theorem}{Smooth uniformity - finite space $X$.}

$E(f(L))=\int_0^1 f(L) dL-\varepsilon$, where $|\varepsilon| \leq \max|f''| \cdot \max |\omega|^2 / 24$.

Since the definition of luck only depends on the probabilities of outcomes, it is natural to consider the set of equivalence classes $[x]$ from $[X]$ defined by equal probabilities: $[x]=\{y \mid p(y)=p(x) \}$.  

We also use the midpoint integration estimate:
\begin{equation*}
\int_{a}^{b} f(L) \, dL = (b-a)f(\frac{a+b}{2})+\frac{(b-a)^3}{24} f''(\xi), \text{where $a < \xi < b$.}
\end{equation*}

\begin{align}
\int_0^1 f(L) \, dL 
  &= \sum_{[x]} \int_{L([x])-\frac{1}{2}|\omega([x])|}^{L([x])+\frac{1}{2}|\omega([x])|} f(L) \, dL \,, \\
  &= \sum_{[x]} \left\{ |\omega([x])| f(L([x])) + \frac{|\omega([x])|^3}{24} f''(\xi_{[x]}) \right\} \,, \\
  &= \sum_{x} p(x) f(L(x)) - \varepsilon \,, \\
  &= E(f(L)) -\varepsilon \,, \\
\varepsilon 
\label{eq:midpointerror}
   &= \sum_{[x]} \frac{|\omega([x])|^3}{24} f''(\xi_{[x]}) \,, \\
 |\varepsilon| 
   &\leq \frac{1}{24} \max_{x} |\omega(x)|^2 \cdot \max_{0\leq L \leq 1} |f''(L)| \,.
\end{align}
\end{theorem}

\begin{theorem}{Moments - finite space $X$.}

For $p \geq 1$, $E(L^p)=1/(p+1)-\varepsilon$, $0 \leq \varepsilon \leq p \cdot (p-1) \max |\omega|^2/24$.

Proof.  This is an example of $f(L)=L^p$ with $p \geq 1$ in the theorem above, and noting that $f''(L)$ is non-negative, so $\varepsilon$ in (\ref{eq:midpointerror}) must be non-negative.
\end{theorem}

\begin{theorem}{Interval uniformity - finite space $X$.}

For $0 \leq a \leq b \leq 1$, $E(L \in [a,b])=b-a - \varepsilon$, $|\varepsilon| \leq \max |\omega|$.

Proof.  Let $f(L)$ be the characteristic function for the closed interval $[a,b]$.
\begin{align}
b-a &=\int_0^1 f(L) \, dL  \\
  &= \sum_{[x]} \int_{L([x])-\frac{1}{2}|\omega([x])|}^{L([x])+\frac{1}{2}|\omega([x])|} f(L) \, dL \,, \\
  &= E(f(L)) + \varepsilon \,, \\
\varepsilon &= \sum_{[x]} \int_{L([x])-\frac{1}{2}|\omega([x])|}^{L([x])+\frac{1}{2}|\omega([x])|} (f(L)-f(L([x]))) \, dL \,.
\end{align}
Now in the sum that defines $\varepsilon$, there are at most 2 discontinuities in an otherwise constant 0, 1 or -1 integrand.  If none occur in an interval, that term is exactly zero.  If one occurs, the integrand is zero for at least $\frac{1}{2}$ of the interval, so that error is at most $\frac{1}{2}|\omega([x])|$, and can be only once more in one other interval (with the same kind of error).  And if 2 occur, they occur nowhere else and the error is bounded by $|\omega([x])|$.  In each case we have our theorem.
\end{theorem}

\section{From Computation}

In this section, we again consider only a finite probability space $X$ with $|X|$ elements $x_k$, $k=1,\ldots,|X|$ and probabilities $p_k=p(x_k)$.

Let $S=(s_1, \ldots, s_n)$ be $|S|$ independent samples taken from $X$.  We define an estimator for $L(x)$ as:
\begin{align*}
 \ell(x) = & \frac{1}{|S|} \left\{\text{\# of outcomes in S more probable than $x$}\right\}  \\
 & + \frac{1}{2|S|} \left\{\text{\# of outcomes in S equally probable to $x$}\right\} 
\end{align*}
We asserted that
\begin{align}
\label{eq:elmeanx}
E(\ell(x))&=L(x) \,. \\
\label{eq:elvariancex}
E([\ell(x)-L(x)]^2) &= \frac{1}{|S|} \left[L(x) \cdot (1-L(x)) -\frac{1}{4}|\omega(x)|\right] \,.
\end{align}

For the remainder of this section, we consider $x \in X$ to be a fixed choice, and so drop the $\cdot (x)$ functional notation, which would otherwise pepper every equation.  This way (\ref{eq:elmeanx}-\ref{eq:elvariancex}) becomes
\begin{align}
\label{eq:elmean}
E(\ell)&=L \,. \\
\label{eq:elvariance}
E([\ell-L]^2) &= \frac{1}{|S|} \left[L \cdot (1-L) -\frac{1}{4}|\omega|\right] \,
\end{align}
The point of this section is to work out the details of these facts.

The definition of $\ell$ does not depend on the ordering of the sample $S$, so the vector of counts $(c_1,\ldots,c_{|X|})$ is equivalent information about the sample for our purposes.  We define $c$ as the vector of counts of elements in $S$, so that $x_k$ occurs $c_k$ times in the sample $S$.

The probability of obtaining a sample counts $c$ is given by the multinomial distribution
\begin{equation}
P(c)=\left(\begin{array}{c} n \\ c \end{array}\right) p^c 
  = \frac{n!}{c_1! \cdot \cdots \cdot c_{|X|}!} p_1^{c_1} \cdots p_{|X|}^{c_{|X|}} \,, \text{where $n=\sum c=|S|$.}
\end{equation}
Here are some well-known moments of the multinomial distribution (the sum is over all samples with $\sum c = n$):
\begin{align}
\sum_c P(c) &= 1 \,, \\
\sum_c P(c) c &= n p \,, \\
\sum_c P(c) c c^T &= n \diag(p) + (n^2-n) p p^T \,.
\end{align}
Here $c$ and $p$ are $|X| \times 1$ column vectors, and $\diag(p)$ is the diagonal $|X| \times |X|$ matrix with the probabilities $p$ along the diagonal.

It is useful to introduce $A$ and $a$, which are analogous to $\Omega$ and $\omega$ (recall that $x$ is fixed in this discussion):

\begin{align}
A &= \left\{ k \mid p(x_k) > p(x) \right\} \,, \\
  &= \left\{ k \mid x_k \in \Omega \right\} \,, \\
|A| &= \frac{1}{n} \sum_{k \in A} c_k \,.
\end{align}

Note that $A$ is the index set of $\Omega$, and so independent of $c$, but $|A|$ depends on the sample counts.  By contrast, both $\Omega$ and $|\Omega|$ are constant (properties of the entire probability space) for a fixed choice of $x$.

Similarly,
\begin{align}
a &= \left\{ k \mid p(x_k) = p(x) \right\} \,, \\
  &= \left\{ k \mid x_k \in \omega \right\} \,, \\
|a| &= \frac{1}{n} \sum_{k \in a} c_k \,.
\end{align}

Using the well-known multinomial moment results, we can compute some useful expected values of $|A|$ and $|a|$:

First $E(|A|)=|\Omega|$
\marginnote{Similarly $E(|a|)=|\omega|$.}:
\begin{align}
E(|A|)&=\sum_c P(c) \frac{1}{n} \sum_{k\in A} c_k \\
      &= \frac{1}{n} \sum_{k\in A} \sum_c P(c) c_k \\
      &= \frac{1}{n} \sum_{k\in A} \sum_c P(c) c_k \\
      &= \frac{1}{n} \sum_{k\in A} n p_k \\
      &= |\Omega| \,.
\end{align}


Next $E(|A|^2)=\frac{1}{n} |\Omega| \cdot (1-|\Omega|) + |\Omega|^2$
\marginnote{Similarly, $E(|a|^2)=\frac{1}{n} |\omega| \cdot (1-|\omega|) + |\omega|^2$.}:
\begin{align}
E(|A|^2) &= \frac{1}{n^2} \sum_{k,k' \in A} \sum_c P(c) c_k c_{k'} \,, \\
         &= \frac{1}{n^2} \sum_{k,k' \in A} \left[ n \diag(p) + (n^2+n) p p^T \right]_{k,k'} \,, \\
         &= \frac{1}{n} |\Omega| + \frac{n-1}{n} |\Omega|^2 \,, \\
         &= \frac{1}{n} |\Omega| \cdot (1-|\Omega|) + |\Omega|^2 \,.
\end{align}

We also will need the cross term $E(|A||a|)$:
\begin{align}
E(|A||a|) &= \frac{1}{n^2} \sum_{k \in A,k' \in a} \sum_c P(c) c_k c_{k'} \,, \\
         &= \frac{1}{n^2} \sum_{k \in A, k' \in a} \left[ n \diag(p) + (n^2+n) p p^T \right]_{k,k'} \,, \\
         &=  \frac{n-1}{n} |\Omega||\omega| \,.
\end{align}
Note that the diagonal term $\diag(p)$ contributes nothing in this case because $A$ and $a$ are disjoint sets.

With these preliminaries,
\begin{equation}
\ell = |A| + \frac{1}{2} |a| \,,
\end{equation}

And so
\begin{equation*}
E(\ell)=|\Omega| + \frac{1}{2} |\omega| = L \,.
\end{equation*}
which is the first result.

For the variance,
\begin{align}
E([\ell-L]^2) &= E(\left[(|A|-|\Omega|) + \frac{1}{2}(|a|-|\omega|)\right]^2)  \,, \\
              &= E([|A|-|\Omega|]^2) + E([|A|-|\Omega|][|a|-|\omega|]) + \frac{1}{4} E([|a|-|\omega|]^2) \,, \\
              &= E(|A|^2)-|\Omega|^2 + E(|A||a|)-|\Omega||\omega| + \frac{1}{4} \left\{ E(|a|^2) - |\omega|^2 \right\} \,, \\
\label{eq:compvar1}
              &= \frac{1}{n} \left\{ |\Omega|(1-|\Omega|) -|\Omega||\omega| + \frac{1}{4}|\omega|(1-|\omega|) \right\} \,.
\end{align}
A little algebra will show (\ref{eq:compvar1}) is equal to
\begin{equation}
E([\ell-L]^2) = \frac{1}{n} \left\{ L \cdot (1-L) - \frac{1}{4} |\omega| \right \} \leq \frac{L \cdot (1-L)}{n} \,.
\end{equation}
which is the other thing we wanted to prove.
