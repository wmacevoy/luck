\chapter*{Proofs}


\section{From the introduction}

\begin{theorem}{Range of Luck.} 
For any probability space,
\begin{equation}
0 \leq L(x) \leq 1 \,.
\end{equation}

Proof.  From the definition, $L(x)=|\Omega(x)|+\frac{1}{2} |\omega(x)|$, which is clearly non-negative, and $\Omega(x)$ and $\omega(x)$ are disjoint subsets of the probability space.  Here the $| \cdot |$ notation is the measure of these sets in the probability space, and their union is at most the whole space, so $L(x) \leq |\Omega(x) \cup \omega(x)| \leq 1$.
\end{theorem}

\begin{theorem}{Lucky values.}
If $L(x)$ is close to 1, then $p(x)$ is relatively small and most outcomes have a higher probability (you are lucky).

Proof.  Since $L(x)$ is close to 1, write $L(x)=1-\varepsilon(x)$, where $\varepsilon(x)$ is a small non-negative number.

First, since $\Omega(x)$ and $\omega(x)$ are disjoint subsets of the space of outcomes $X$, $|\Omega(x)|+|\omega(x)| \leq 1$, or 
\begin{equation*}
|\Omega(x)| \leq 1-|\omega(x)| \,.
\end{equation*}

Second, $L(x)=|\Omega(x)|+\frac{1}{2}|\omega(x)|$, which by the first inequality can be bounded as $1-\varepsilon(x)\leq  1-|\omega(x)|+\frac{1}{2}|\omega(x)|$, which can be rearranged as
\begin{equation*}
|\omega(x)| \leq 2 \varepsilon(x) \,.
\end{equation*}

Third, $|\Omega(x)|+\frac{1}{2}|\omega(x)| = 1-\varepsilon(x)$, or $|\Omega(x)|=1-\varepsilon(x)-\frac{1}{2}|\omega(x)|$, which by the second inequality,
\begin{equation}
|\Omega(x)| \geq 1-2 \varepsilon(x) \,.
\end{equation}
Thus at least a $1-2 \varepsilon(x)$ fraction of the probability space have a higher probability of occuring.
\end{theorem}

\begin{theorem}{Unlucky values.}
If $L(x)$ is close to 0, then $p(x)$ is comparatively large, and most outcomes would have a lower probability (you are unlucky).

Proof.  Since $L(x)$ is close to 0, write $L(x)=\varepsilon(x)$, where $\varepsilon(x)$ is a small non-negative number.

First, $L(x)=|\Omega(x)|+\frac{1}{2}|\omega(x)|$, so
\begin{equation*}
\omega(x) \leq 2\varepsilon(x) \,.
\end{equation*}

Second, $|\Omega(x)|+|\omega(x)|=|\Omega(x)|+\frac{1}{2}|\omega(x)|+\frac{1}{2}|\omega(x)| \leq 2\varepsilon(x)$, so
\begin{equation*}
|\Omega(x)|+|\omega(x)| \leq 2\varepsilon(x) \,.
\end{equation*}

Third, $|X-\Omega(x) \cup \omega(x)| \geq 1-|\Omega(x) \cup \omega(x)| \geq 1-2\varepsilon(x)$, or 
\begin{equation*}
|X-\Omega(x) \cup \omega(x)| \geq 1-2\varepsilon(x) \,.
\end{equation*}
Thus at least a $1-2 \varepsilon(x)$ fraction of the probability space have a lower probability of occuring.
\end{theorem}

\begin{theorem}{On average, luck is always 50:50.}

\begin{equation*}
E(L)=\frac{1}{2} \,.
\end{equation*}

Proof.  This is an application of the next theorem where $f(L)=L$.
\end{theorem}

\begin{theorem}{Smooth uniformity - finite space $X$.}

$E(f(L))=\int_0^1 f(L) dL-\varepsilon$, where $|\varepsilon| \leq \max|f''| \cdot \max |\omega|^2 / 24$.

Since the definition of luck only depends on the probabilities of outcomes, it is natural to consider the set of equivalence classes $[x]$ from $[X]$ defined by equal probabilities: $[x]=\{y \mid p(y)=p(x) \}$.  

We also use the midpoint integration estimate:
\begin{equation*}
\int_{a}^{b} f(L) \, dL = (b-a)f(\frac{a+b}{2})+\frac{(b-a)^3}{24} f''(\xi), \text{where $a < \xi < b$.}
\end{equation*}

\begin{align}
\int_0^1 f(L) \, dL 
  &= \sum_{[x]} \int_{L([x])-\frac{1}{2}|\omega([x])|}^{L([x])+\frac{1}{2}|\omega([x])|} f(L) \, dL \,, \\
  &= \sum_{[x]} \left\{ |\omega([x])| f(L([x])) + \frac{|\omega([x])|^3}{24} f''(\xi_{[x]}) \right\} \,, \\
  &= \sum_{x} p(x) f(L(x)) - \varepsilon \,, \\
  &= E(f(L)) -\varepsilon \,, \\
\varepsilon 
\label{eq:midpointerror}
   &= \sum_{[x]} \frac{|\omega([x])|^3}{24} f''(\xi_{[x]}) \,, \\
 |\varepsilon| 
   &\leq \frac{1}{24} \max_{x} |\omega(x)|^2 \cdot \max_{0\leq L \leq 1} |f''(L)| \,.
\end{align}
\end{theorem}

\begin{theorem}{Moments - finite space $X$.}

For $p \geq 1$, $E(L^p)=1/(p+1)-\varepsilon$, $0 \leq \varepsilon \leq p \cdot (p-1) \max |\omega|^2/24$.

Proof.  This is an example of $f(L)=L^p$ with $p \geq 1$ in the theorem above, and noting that $f''(L)$ is non-negative, so $\varepsilon$ in (\ref{eq:midpointerror}) must be non-negative.
\end{theorem}

\begin{theorem}{Interval uniformity - finite space $X$.}

For $0 \leq a \leq b \leq 1$, $E(L \in [a,b])=b-a - \varepsilon$, $|\varepsilon| \leq \max |\omega|$.

Proof.  Let $f(L)$ be the characteristic function for the closed interval $[a,b]$.
\begin{align}
b-a &=\int_0^1 f(L) \, dL  \\
  &= \sum_{[x]} \int_{L([x])-\frac{1}{2}|\omega([x])|}^{L([x])+\frac{1}{2}|\omega([x])|} f(L) \, dL \,, \\
  &= E(f(L)) + \varepsilon \,, \\
\varepsilon &= \sum_{[x]} \int_{L([x])-\frac{1}{2}|\omega([x])|}^{L([x])+\frac{1}{2}|\omega([x])|} (f(L)-f(L([x]))) \, dL \,.
\end{align}
Now in the sum that defines $\varepsilon$, there are at most 2 discontinuities in an otherwise constant 0, 1 or -1 integrand.  If none occur in an interval, that term is exactly zero.  If one occurs, the integrand is zero for at least $\frac{1}{2}$ of the interval, so that error is at most $\frac{1}{2}|\omega([x])|$, and can be only once more in one other interval (with the same kind of error).  And if 2 occur, they occur nowhere else and the error is bounded by $|\omega([x])|$.  In each case we have our theorem.
\end{theorem}

\section{From Computation}
In this section, we again consider only a finite probability space $X$ with $|X|$ elements $x_k$, $k=1,\ldots,|X|$ and probabilities $p_k=p(x_k)$.

Let $S=(s_1, \ldots, s_n)$ be $|S|$ independent samples taken from $X$.  We define an estimator for $L(x)$ as:
\begin{align*}
 \ell(x) = & \frac{1}{|S|} \left\{\text{\# of outcomes in S more probable than $x$}\right\}  \\
 & + \frac{1}{2|S|} \left\{\text{\# of outcomes in S equally probable to $x$}\right\} 
\end{align*}
We asserted that
\begin{align}
\label{eq:elmeanx}
E(\ell(x))&=L(x) \,. \\
\label{eq:elvariancex}
E([\ell(x)-L(x)]^2) &= \frac{1}{|S|} \left[L(x) \cdot (1-L(x)) -\frac{1}{4}|\omega(x)|\right] \,.
\end{align}

For the remainder of this section, we consder $x \in X$ to be a fixed choice, and so drop the $\cdot (x)$ functional notation, which would otherwise pepper every equation.  This way (\ref{eq:elmeanx}-\ref{eq:elvariancex}) becomes
\begin{align}
\label{eq:elmean}
E(\ell)&=L \,. \\
\label{eq:elvariance}
E([\ell-L]^2) &= \frac{1}{|S|} \left[L \cdot (1-L) -\frac{1}{4}|\omega|\right] \,
\end{align}

The definition of $\ell$ does not depend on the ordering of the sample $S$, so the vector of counts $(c_1,\ldots,c_{|X|})$ is equivalent information about the sample for our purposes.  We define $c$ as the vector of counts of elements in $S$, so that $x_k$ occurs $c_k$ times in the sample $S$.

The probability of obtaining a sample counts $c$ is given by the multinomial distribution
\begin{equation}
P(c)=\left(\begin{array}{c} n \\ c \end{array}\right) p^c 
  = \frac{n!}{c_1! \cdot \cdots \cdot c_n!} p_1^{c_1} \cdots p_n^{c_n} \,, \text{where $n=\sum c=|S|$.}
\end{equation}
Here are some well-known moments of the multinomial distribution (the sum is over all samples with $\sum c = n$):
\begin{align}
\sum_c P(c) &= 1 \,, \\
\sum_c P(c) c &= n p \,, \\
\sum_c P(c) c c^T &= n \diag(p) + (n^2-n) p p^T \,.
\end{align}
Here $c$ and $p$ are taken as column vectors, and $\diag(p)$ is the diagonal $n \times n$ matrix with the probabilites $p$ along the diagonal.


